"""Run Bayesian analysis of data from a paired-comparison experiment.
This script should be used as a template,
to be copied and modified for any particular experiment.

*** Usage, four main steps, see also explicit template example below

*1: Create a PairedCompFrame instance to define experiment and select input data.

*2: Load a set of test results

*3: Learn Bayesian model corresponding to observed data
using either Thurstone Case V or Bradley-Terry-Luce probabilistic choice model.

*4: Display results and save figures and tables to a directory tree
"""

from pathlib import Path
import pickle
import logging

from PairedCompCalc import pc_logging, __version__
from PairedCompCalc.pc_data import PairedCompFrame, PairedCompDataSet
from PairedCompCalc.pc_model import PairedCompResultSet, Thurstone, Bradley

import PairedCompCalc.pc_display as shw

# -----------------------------------
# model_class = Bradley
model_class = Thurstone
# = selected class of probabilistic choice model for the analysis

# ---------------------- location of input and output data:
work_path = Path.home() / 'Documents' / 'PairedComp_sim'  # or whatever...

data_path = work_path / 'data'
# = directory to be searched for read-only input data files

result_path = work_path / 'result'
# = top directory for all result files,
# with sub-directories created as needed.
# NOTE: Existing result files in this directory are OVER-WRITTEN WITHOUT WARNING,
# but different analysis runs may add separate files to the same existing directory.

assert result_path != data_path, 'Result directory must be different from input data directory'

model_result_file = 'pc_result.pkl'
# = name of file with saved PairedCompResultSet instance, if used

display_file = 'pc_displays.pkl'
# = name of file with saved PairedCompDisplaySet instance, if used

log_file = 'run_pc_log.txt'
# = name of log file

result_path.mkdir(parents=True, exist_ok=True)

pc_logging.setup(result_path, log_file)
logging.info(f'*** Running PairedCompCalc version {__version__}')

# ---------------------- Example, defining experimental structure:

# pcf = PairedCompFrame(attributes=['Speech Clarity', 'Pleasantness', 'Preference'],
#                       # objects=['Program0', 'Program1', 'Program2'], # may be taken from input data
#                       objects_alias=['A', 'B', 'C', 'D'],  # to hide real object names in displays
#                       forced_choice=False,
#                       difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
#                       test_factors={'Background': ['Quiet', 'Noisy']}
#                       )

# Test example for data generated by run_sim.py:
pcf = PairedCompFrame(attributes=['SimQ'],
                      # objects=[f'HA{i}' for i in range(3)],  # all input data used
                      forced_choice=False,
                      difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
                      test_factors={'Stim': ['speech', 'music'], 'SNR': ['Quiet', 'High']}
                      )


# ---------------------- main analysis work:

logging.info(f'Analysing paired-comparison data in {data_path}')

# Using xlsx data files, for example:
# ds = PairedCompDataSet.load(pcf, data_path,
#                             fmt='xlsx',
#                             groups=['A', 'B'],
#                             sheets=[f'Subject{i}' for i in range(10)],
#                             subject='sheet',
#                             top_row=2,
#                             attribute='A',  # column address
#                             pair=('B', 'C'),  # column address
#                             difference='D',  # column address
#                             choice='E',  # column address
#                             Sound='G'  # column address for category of test-factor Sound
#                             )
# *** See pc_file_xlsx for more details about the xlsx input file format

# OR, using files formatted as pc_file_json.PairedCompFile:
# e.g., data files generated by run_sim.py:
ds = PairedCompDataSet.load(pcf, data_path, groups=['Group0'], fmt='json')

logging.info(f'Learning Results with model {model_class}')

pc_result = PairedCompResultSet.learn(ds, rv_class=model_class)

# ------------------------------- Optionally, dump learned result set:
with (result_path / model_result_file).open('wb') as f:
    pickle.dump(pc_result, f)

# may be re-loaded to save learning time,
# in case different display formats are desired later.

# ------------------------------- generate result displays:

pc_display_set = shw.display(pc_result)
# = default display combination, showing estimated results for
# (1) random individual in the population from which participants were recruited,
# (2) the population mean.
# OR
# pc_display_set = shw.display(pc_result,
#                          percentiles=[2.5, 50., 97.5],
#                          credibility_limit=0.8,
#                          show_intervals=False,
#                          table_format='tab')
# *** See pc_display.FMT and pc_display_format.FMT for available format parameters

# ------------------------------------------------------------------------------
# Alternative: display other combinations of predictive distributions:

# logging.info('Displaying predictive_group_individual and predictive_population_individual')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual(),
#                                               pc_result.predictive_population_individual())

# OR
# logging.info('Displaying predictive_group_individual and predictive_population_mean')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual(),
#                                               pc_result.predictive_population_mean())

# OR
# logging.info('Displaying only predictive_group_individual')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual())

# OR other single predictive distribution or pair of predictive distributions.

# ***** Edit display plots or tables here, if needed *****
# Each display element can be accessed and modified by the user, before saving,

# plt.show()
# to show all figures on screen before saving,
# NOTE: This blocks the program until figure windows are closed by user

# -------------- Optional approximate Likelihood Ratio significance test:
# -------------- NOTE: Training the NULL model takes same time as the actual model.

logging.info(f'Running Likelihood Ratio Significance Test. ' +
             'Null Hypothesis: Population mean quality parameters all equal.')
logging.info('*** NOTE: Likelihood-Ratio p-values are approximate ***')
logging.info(f'Learning Null-hypothesis model with {model_class}')

pc_result_null = PairedCompResultSet.learn(ds, rv_class=model_class, null_quality=True)
pc_display_set.likelihood_ratio_test(pc_result_null, pc_result)


# ------------------------------- save all result displays:
pc_display_set.save(result_path)

# Optionally, save the display set as a single editable object:
# if display_file is not None:
#     with (result_path / display_file).open('wb') as f:
#         pickle.dump(pc_display_set, f)

# *** can be loaded again, elements edited, and re-saved

logging.info(f'All results saved in {result_path} and sub-dirs.')
logging.shutdown()
