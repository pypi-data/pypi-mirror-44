notify_audit: false
locked: false
name: '[${environment}] disk read ops being throttled on host'
tags: ['${environment}', reactiveops]
include_tags: false
no_data_timeframe: null
silenced: {}
new_host_delay: 600
require_full_window: true
notify_no_data: false
renotify_interval: 0
evaluation_delay: 60
escalation_message: ''
query: min(last_2h):(avg:gcp.gce.instance.disk.throttled_read_ops_count{kubernetescluster:${cluster}} by {host}.rollup(avg, 300) / avg:gcp.gce.instance.disk.read_ops_count{kubernetescluster:${cluster}} by {host}.rollup(avg, 300) ) * 100 > 20
message: |
  This metric indicates disk reads are being throttled. This can result in
    - timeouts during image pulls
    - pod startup failure due to longer startups failing livenessprobe expectations
  Are there lots of pods with large images on this node?
  Are one or more pods using more disk read ops than expected?
  ${notifications}
type: query alert
thresholds: {critical: 30, warning: 20, critical_recovery: 15}
timeout_h: 0
